{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b07263-90c1-48aa-bf07-19ebab64800c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt = \"What are symptoms of lung cancer?\"\\nresponse = ollama.chat(\\n    model=\\'llama2\\',\\n    messages=[{\\'role\\': \\'user\\', \\'content\\': prompt}]\\n)\\n\\nprint(response[\\'message\\'][\\'content\\'])\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!ollama start\n",
    "#!ollama run llama2\n",
    "#monotykamary/medichat-llama3\n",
    "'''\n",
    "prompt = \"What are symptoms of lung cancer?\"\n",
    "response = ollama.chat(\n",
    "    model='llama2',\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174dcfa6-9a65-4ea8-83af-7f227233fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server started\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Start the Ollama server\n",
    "server_process = subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "print(\"Ollama server started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d9cdd9-9cc9-46ca-9b31-d42a0a87b80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find '/home/workbench/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEjrrZNoMuOXlrabeYs3XqexD51+eJ96NDmnnVGf2wGh\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/15 01:02:46 routes.go:1189: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/workbench/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2024-11-15T01:02:46.048Z level=INFO source=images.go:755 msg=\"total blobs: 0\"\n",
      "time=2024-11-15T01:02:46.048Z level=INFO source=images.go:762 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-11-15T01:02:46.049Z level=INFO source=routes.go:1240 msg=\"Listening on 127.0.0.1:11434 (version 0.4.1)\"\n",
      "time=2024-11-15T01:02:46.049Z level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama1387917143/runners\n",
      "time=2024-11-15T01:02:46.136Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[rocm cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12]\"\n",
      "time=2024-11-15T01:02:46.137Z level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 15 01:02:46 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.58.02              Driver Version: 556.12         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   52C    P8              3W /  135W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce4366d-5b13-4bd2-adc7-2d09ab427dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-11-15T01:02:46.498Z level=INFO source=types.go:123 msg=\"inference compute\" id=GPU-ce3fa193-1955-92ed-8b2b-22788386623b library=cuda variant=v12 compute=8.9 driver=12.5 name=\"NVIDIA GeForce RTX 4060 Laptop GPU\" total=\"8.0 GiB\" available=\"6.9 GiB\"\n",
      "time=2024-11-15T01:02:48.104Z level=INFO source=download.go:175 msg=\"downloading 8934d96d3f08 in 16 239 MB part(s)\"\n",
      "time=2024-11-15T01:08:43.958Z level=INFO source=download.go:175 msg=\"downloading 8c17c2ebb0ea in 1 7.0 KB part(s)\"\n",
      "time=2024-11-15T01:08:46.248Z level=INFO source=download.go:175 msg=\"downloading 7c23fb36d801 in 1 4.8 KB part(s)\"\n",
      "time=2024-11-15T01:08:48.143Z level=INFO source=download.go:175 msg=\"downloading 2e0493f67d0c in 1 59 B part(s)\"\n",
      "time=2024-11-15T01:08:50.065Z level=INFO source=download.go:175 msg=\"downloading fa304d675061 in 1 91 B part(s)\"\n",
      "time=2024-11-15T01:08:52.079Z level=INFO source=download.go:175 msg=\"downloading 42ba7f8a01dd in 1 557 B part(s)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/11/15 - 01:08:55 | 200 |          6m8s |       127.0.0.1 | POST     \"/api/pull\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.pull(model=\"llama2\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c087d4-0ada-4fef-a462-76436f3c793a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Client._stream at 0x7f527cdf5070>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.pull(model=\"llama2\", stream=True)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f42c8ff5-bbaf-4444-a260-2dbdfeef7524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'pulling manifest'}\n",
      "{'status': 'pulling 8934d96d3f08', 'digest': 'sha256:8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246', 'total': 3826781184, 'completed': 3826781184}\n",
      "{'status': 'pulling 8c17c2ebb0ea', 'digest': 'sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b', 'total': 7020, 'completed': 7020}\n",
      "{'status': 'pulling 7c23fb36d801', 'digest': 'sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d', 'total': 4766, 'completed': 4766}\n",
      "{'status': 'pulling 2e0493f67d0c', 'digest': 'sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988', 'total': 59, 'completed': 59}\n",
      "{'status': 'pulling fa304d675061', 'digest': 'sha256:fa304d6750612c207b8705aca35391761f29492534e90b30575e4980d6ca82f6', 'total': 91, 'completed': 91}\n",
      "{'status': 'pulling 42ba7f8a01dd', 'digest': 'sha256:42ba7f8a01ddb4fa59908edd37d981d3baa8d8efea0e222b027f29f7bcae21f9', 'total': 557, 'completed': 557}\n",
      "{'status': 'verifying sha256 digest'}\n",
      "{'status': 'writing manifest'}\n",
      "[GIN] 2024/11/15 - 01:08:55 | 200 |  438.169447ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "{'status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "for val in response:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce35a14-c62e-46bf-b7d5-6293c67b6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-11-15T01:08:55.718Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/home/workbench/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 gpu=GPU-ce3fa193-1955-92ed-8b2b-22788386623b parallel=1 available=7443841024 required=\"5.2 GiB\"\n",
      "time=2024-11-15T01:08:55.800Z level=INFO source=server.go:105 msg=\"system memory\" total=\"15.5 GiB\" free=\"14.0 GiB\" free_swap=\"4.0 GiB\"\n",
      "time=2024-11-15T01:08:55.801Z level=INFO source=memory.go:343 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[6.9 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.2 GiB\" memory.required.partial=\"5.2 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[5.2 GiB]\" memory.weights.total=\"4.4 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"102.6 MiB\" memory.graph.full=\"164.0 MiB\" memory.graph.partial=\"193.0 MiB\"\n",
      "time=2024-11-15T01:08:55.801Z level=INFO source=server.go:383 msg=\"starting llama server\" cmd=\"/tmp/ollama1387917143/runners/cuda_v12/ollama_llama_server --model /home/workbench/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 2048 --batch-size 512 --n-gpu-layers 33 --threads 16 --parallel 1 --port 45055\"\n",
      "time=2024-11-15T01:08:55.801Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
      "time=2024-11-15T01:08:55.803Z level=INFO source=server.go:562 msg=\"waiting for llama runner to start responding\"\n",
      "time=2024-11-15T01:08:55.803Z level=INFO source=server.go:596 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
      "time=2024-11-15T01:08:55.890Z level=INFO source=runner.go:863 msg=\"starting go runner\"\n",
      "time=2024-11-15T01:08:55.890Z level=INFO source=runner.go:864 msg=system info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=16\n",
      "time=2024-11-15T01:08:55.890Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:45055\"\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/workbench/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes\n",
      "time=2024-11-15T01:08:56.054Z level=INFO source=server.go:596 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.14 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "time=2024-11-15T01:08:57.057Z level=INFO source=server.go:601 msg=\"llama runner started in 1.26 seconds\"\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/workbench/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 1\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = all F32\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llama_model_load: vocab only - skipping tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/11/15 - 01:09:09 | 200 | 14.258878934s |       127.0.0.1 | POST     \"/api/generate\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.generate(model=\"llama2\", prompt=\"What are the symptoms of lung cancer?\")\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc175e2-bf1c-4707-b42d-0a5c20d8ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lung cancer can cause a variety of symptoms, which can vary depending on the type and stage of the disease. Here are some common symptoms of lung cancer:\n",
      "\n",
      "1. Coughing: A persistent cough that does not go away or worsens over time is a common symptom of lung cancer. The cough may be painful and accompanied by bloody phlegm.\n",
      "2. Chest pain: Pain in the chest or shoulder that worsens with deep breathing or coughing can be a sign of lung cancer.\n",
      "3. Shortness of breath: As lung cancer grows, it can reduce the amount of space in the chest for the heart to fill with blood, leading to shortness of breath or wheezing.\n",
      "4. Fatigue: Feeling tired or weak without any obvious reason can be a symptom of lung cancer.\n",
      "5. Weight loss: Unexplained weight loss is common in people with lung cancer, especially if they are experiencing unexplained fatigue or fever.\n",
      "6. Coughing up blood: Coughing up blood or rust-colored sputum can be a symptom of lung cancer, particularly if it occurs frequently or with no apparent cause.\n",
      "7. Chest tightness: Feeling like there is a band around the chest or tightness in the chest can be a sign of lung cancer.\n",
      "8. Hoarseness: A persistent cough or hoarseness that does not go away can be a symptom of lung cancer, particularly if it occurs with other symptoms such as chest pain or weight loss.\n",
      "9. Headaches: Some people with lung cancer may experience headaches due to the spread of cancer cells to the brain.\n",
      "10. Bone pain: Pain in the bones, particularly in the back or hips, can be a symptom of lung cancer that has spread to the bones.\n",
      "11. Numbness or weakness: Weakness or numbness in the arms or legs can be a sign of lung cancer that has spread to the brain or spinal cord.\n",
      "12. Swelling: Swelling in the face, neck, or underarm area can occur when lung cancer spreads to the lymph nodes.\n",
      "13. Night sweats: Unexplained night sweats can be a symptom of lung cancer, particularly if they occur with other symptoms such as weight loss or fever.\n",
      "14. Hemoptysis: Coughing up blood or rust-colored sputum can be a sign of lung cancer, particularly if it occurs frequently or with no apparent cause.\n",
      "15. Recurrent respiratory infections: People with lung cancer may experience recurrent respiratory infections, such as bronchitis or pneumonia, due to the weakened immune system.\n",
      "\n",
      "It's important to note that these symptoms can also be caused by other conditions, and only a doctor can diagnose lung cancer based on a thorough medical evaluation and diagnostic tests. If you are experiencing any of these symptoms, it is essential to see a doctor promptly for an accurate diagnosis and appropriate treatment.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00c0b49-fea3-495f-8498-a4087cb744c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-11-15T01:15:40.257Z level=INFO source=download.go:175 msg=\"downloading abf30c3bb477 in 16 307 MB part(s)\"\n",
      "time=2024-11-15T01:24:13.196Z level=INFO source=download.go:175 msg=\"downloading 555a4648ca93 in 1 142 B part(s)\"\n",
      "time=2024-11-15T01:24:15.157Z level=INFO source=download.go:175 msg=\"downloading 1304084d3201 in 1 230 B part(s)\"\n",
      "time=2024-11-15T01:24:17.108Z level=INFO source=download.go:175 msg=\"downloading d827af7436fe in 1 12 KB part(s)\"\n",
      "time=2024-11-15T01:24:18.978Z level=INFO source=download.go:175 msg=\"downloading ade69533d749 in 1 36 B part(s)\"\n",
      "time=2024-11-15T01:24:20.909Z level=INFO source=download.go:175 msg=\"downloading 652439d71178 in 1 561 B part(s)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/11/15 - 01:24:24 | 200 |         8m45s |       127.0.0.1 | POST     \"/api/pull\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.pull(model=\"monotykamary/medichat-llama3\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d790725-dfd6-442a-8f08-45fe5b75ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Explain to the patient their next steps to treating their lung cancer.\"\n",
    "\n",
    "response = ollama.generate(model=\"monotykamary/medichat-llama3\",\n",
    "                           stream=True,\n",
    "                           system=\"Act as if you are a doctor that is providing advice to a patient diagnosed with lung cancer.\",\n",
    "                           prompt=input(\"Ask our Health Chatbot any pertaining medical inquiries regarding diagnosis or etc.\"))\n",
    "\n",
    "tokens = [token[\"response\"] for token in response]\n",
    "print(\"\".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e8efd-6712-46a3-b2da-b5009f8fa5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f337820-d631-4f5a-8cb3-88366b13f12b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
