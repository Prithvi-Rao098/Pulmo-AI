{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b07263-90c1-48aa-bf07-19ebab64800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/11/14 08:59:49 routes.go:1189: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/workbench/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2024-11-14T08:59:49.227Z level=INFO source=images.go:755 msg=\"total blobs: 6\"\n",
      "time=2024-11-14T08:59:49.227Z level=INFO source=images.go:762 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-11-14T08:59:49.228Z level=INFO source=routes.go:1240 msg=\"Listening on 127.0.0.1:11434 (version 0.4.0)\"\n",
      "time=2024-11-14T08:59:49.229Z level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama2424379676/runners\n",
      "time=2024-11-14T08:59:49.305Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm cpu]\"\n",
      "time=2024-11-14T08:59:49.305Z level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\n",
      "time=2024-11-14T08:59:49.323Z level=INFO source=gpu.go:386 msg=\"no compatible GPUs were discovered\"\n",
      "time=2024-11-14T08:59:49.323Z level=INFO source=types.go:123 msg=\"inference compute\" id=0 library=cpu variant=avx2 compute=\"\" driver=0.0 name=\"\" total=\"15.5 GiB\" available=\"13.8 GiB\"\n"
     ]
    }
   ],
   "source": [
    "!ollama start\n",
    "!ollama run llama2\n",
    "#monotykamary/medichat-llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4366d-5b13-4bd2-adc7-2d09ab427dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "prompt = \"What are symptoms of lung cancer?\"\n",
    "response = ollama.chat(\n",
    "    model='llama2',\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134fa26-9821-4bed-b5b2-f8c7997a5469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
