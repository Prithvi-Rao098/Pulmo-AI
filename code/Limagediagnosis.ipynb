{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b130f7-04db-4504-95fa-0dba7718c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnosis of lung damage based on medical imagery .ipynb Prithvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74db76a-46f6-40da-b2f9-8dfa4076a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: monai in /home/workbench/.local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: torch>=1.9 in /home/workbench/.local/lib/python3.10/site-packages (from monai) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/workbench/.local/lib/python3.10/site-packages (from monai) (2.1.1)\n",
      "Requirement already satisfied: filelock in /home/workbench/.local/lib/python3.10/site-packages (from torch>=1.9->monai) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/workbench/.local/lib/python3.10/site-packages (from torch>=1.9->monai) (3.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/workbench/.local/lib/python3.10/site-packages (from torch>=1.9->monai) (1.13.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/workbench/.local/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/workbench/.local/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/workbench/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/workbench/.local/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/workbench/.local/lib/python3.10/site-packages (from matplotlib) (2.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/workbench/.local/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/workbench/.local/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/workbench/.local/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/workbench/.local/lib/python3.10/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/workbench/.local/lib/python3.10/site-packages (2.1.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/workbench/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/workbench/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/workbench/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/workbench/.local/lib/python3.10/site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python-headless in /home/workbench/.local/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/workbench/.local/lib/python3.10/site-packages (from opencv-python-headless) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "# install necessary models, datasets, and libraries\n",
    "!pip install monai\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c326da0-a776-409c-8bb9-f438c28c384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.2\n",
      "Numpy version: 2.1.1\n",
      "Pytorch version: 2.0.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 59a7211070538586369afd4a01eca0a7fe2e742e\n",
      "MONAI __file__: /home/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.15.2\n",
      "tqdm version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.6\n",
      "pandas version: 2.2.3\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.networks.nets import DenseNet121\n",
    "from monai.transforms import *\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.utils import set_determinism\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56485f25-14e0-4c14-bede-dd197f8dadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in the already imported data from the data directory\n",
    "train_directory='../data/ctimages/Data/train'\n",
    "val_directory='../data/ctimages/Data/valid'\n",
    "test_directory='../data/ctimages/Data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "554b0742-9e61-4f90-b632-7e91be0b8e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'normal', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa']\n"
     ]
    }
   ],
   "source": [
    "# Get sorted class names from the train dataset\n",
    "class_names = sorted(os.listdir(train_directory))\n",
    "num_class = len(class_names)\n",
    "\n",
    "# Create a list of all image files with their corresponding labels\n",
    "image_file_list, image_label_list = [], []\n",
    "\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(train_directory, class_name)\n",
    "    image_files = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n",
    "    \n",
    "    image_file_list.extend(image_files)\n",
    "    image_label_list.extend([label] * len(image_files))\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68593008-e48f-4bd4-b6d4-c8480ed83ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'normal', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa']\n"
     ]
    }
   ],
   "source": [
    "# List and sort validation class names\n",
    "v_class_names = sorted(os.listdir(val_directory))\n",
    "v_num_class = len(v_class_names)\n",
    "\n",
    "# Initialize lists for image file paths and labels\n",
    "v_image_file_list, v_image_label_list = [], []\n",
    "\n",
    "# Iterate over each class and collect image file paths and corresponding labels\n",
    "for label, class_name in enumerate(v_class_names):\n",
    "    class_dir = os.path.join(val_directory, class_name)\n",
    "    image_files = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n",
    "    \n",
    "    v_image_file_list.extend(image_files)\n",
    "    v_image_label_list.extend([label] * len(image_files))\n",
    "\n",
    "print(v_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6535dc86-5699-4c09-a53f-a84ee8a0d6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adenocarcinoma', 'large.cell.carcinoma', 'normal', 'squamous.cell.carcinoma']\n"
     ]
    }
   ],
   "source": [
    "# List and sort test class names\n",
    "t_class_names = sorted(os.listdir(test_directory))\n",
    "t_num_class = len(t_class_names)\n",
    "\n",
    "# Initialize lists for image file paths and labels\n",
    "t_image_file_list, t_image_label_list = [], []\n",
    "\n",
    "# Iterate over each class and collect image file paths and corresponding labels\n",
    "for label, class_name in enumerate(t_class_names):\n",
    "    class_dir = os.path.join(test_directory, class_name)\n",
    "    image_files = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n",
    "    \n",
    "    t_image_file_list.extend(image_files)\n",
    "    t_image_label_list.extend([label] * len(image_files))\n",
    "\n",
    "print(t_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c71d7576-8194-475c-af83-f3dd18a957c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training count = 613 Validation count = 72 Test count = 315\n"
     ]
    }
   ],
   "source": [
    "# prepare training, validation, and testing datasets\n",
    "trainX=np.array(image_file_list)\n",
    "trainY=np.array(image_label_list)\n",
    "valX=np.array(v_image_file_list)\n",
    "valY=np.array(v_image_label_list)\n",
    "testX=np.array(t_image_file_list)\n",
    "testY=np.array(t_image_label_list)\n",
    "\n",
    "print(\"Training count =\",len(trainX),\"Validation count =\", len(valX), \"Test count =\",len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9752d962-388b-41cc-a365-3de75d4917a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumDimension(Transform):\n",
    "    def __init__(self, dim=1):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return inputs.sum(self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93fc98d1-0729-42c0-b77c-29513ee8ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResize(Transform):\n",
    "    def __init__(self, size=(100, 100)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        image = cv2.resize(inputs.numpy(), dsize=self.size, interpolation=cv2.INTER_CUBIC)\n",
    "        return torch.tensor(image, dtype=torch.float32)  # Convert back to tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f75e8f-e850-4019-8123-122550a42fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Astype(Transform):\n",
    "    def __init__(self, dtype=torch.float32):  # Use torch data types\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return inputs.to(self.dtype)  # Convert tensor to specified type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "339e6b8b-ddf0-4792-8112-6ddb6a946db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    LoadImage(image_only=True),\n",
    "    Resize((-1,1)),\n",
    "    Astype(),\n",
    "    SumDimension(2),\n",
    "    Astype(),\n",
    "    MyResize(),\n",
    "    #AddChannel(),    \n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImage(image_only=True),\n",
    "    Resize((-1,1)),\n",
    "    Astype(),\n",
    "    SumDimension(2),\n",
    "    Astype(),\n",
    "    MyResize(),\n",
    "    #AddChannel(),    \n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "act = Activations(softmax=True)\n",
    "to_onehot = AsDiscrete(to_onehot=num_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef2a57a7-ec0e-4de5-bf08-b99d724386ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedNISTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_files, labels, transforms):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transforms(self.image_files[index]), self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a4ac5eb-83ef-4d13-b6fa-3f851f5d2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MedNISTDataset(trainX, trainY, train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "val_ds = MedNISTDataset(valX, valY, val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, num_workers=2)\n",
    "\n",
    "test_ds = MedNISTDataset(testX, testY, val_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94832427-309a-4070-bfb0-a27c8b2a03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = DenseNet121(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=num_class\n",
    ").to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
    "epoch_num = 4\n",
    "val_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecfe48d5-b9e0-483e-8616-6fb3f31f6891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/io/array.py\", line 290, in __call__\n    img = MetaTensor.ensure_torch_and_prune_meta(\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/data/meta_tensor.py\", line 554, in ensure_torch_and_prune_meta\n    img = convert_to_tensor(im, track_meta=get_track_meta() and meta is not None)  # potentially ascontiguousarray\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/utils/type_conversion.py\", line 161, in convert_to_tensor\n    return _convert_tensor(data, dtype=dtype, device=device)\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/utils/type_conversion.py\", line 141, in _convert_tensor\n    tensor = torch.as_tensor(tensor, **kwargs)\nRuntimeError: Could not infer dtype of numpy.float32\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/workbench/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/workbench/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/workbench/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_4806/769140115.py\", line 12, in __getitem__\n    return self.transforms(self.image_files[index]), self.labels[index]\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/compose.py\", line 335, in __call__\n    result = execute_compose(\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/compose.py\", line 111, in execute_compose\n    data = apply_transform(\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.io.array.LoadImage object at 0xffffa49ca500>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     13\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch_data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/io/array.py\", line 290, in __call__\n    img = MetaTensor.ensure_torch_and_prune_meta(\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/data/meta_tensor.py\", line 554, in ensure_torch_and_prune_meta\n    img = convert_to_tensor(im, track_meta=get_track_meta() and meta is not None)  # potentially ascontiguousarray\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/utils/type_conversion.py\", line 161, in convert_to_tensor\n    return _convert_tensor(data, dtype=dtype, device=device)\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/utils/type_conversion.py\", line 141, in _convert_tensor\n    tensor = torch.as_tensor(tensor, **kwargs)\nRuntimeError: Could not infer dtype of numpy.float32\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/workbench/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/workbench/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/workbench/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_4806/769140115.py\", line 12, in __getitem__\n    return self.transforms(self.image_files[index]), self.labels[index]\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/compose.py\", line 335, in __call__\n    result = execute_compose(\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/compose.py\", line 111, in execute_compose\n    data = apply_transform(\n  File \"/home/workbench/.local/lib/python3.10/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.io.array.LoadImage object at 0xffffa49ca500>\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = list()\n",
    "auc_metric = ROCAUCMetric()\n",
    "metric_values = list()\n",
    "for epoch in range(epoch_num):\n",
    "    print('-' * 10)\n",
    "    print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, train_loss: {loss.item():.4f}\")\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [to_onehot(i) for i in y]\n",
    "            y_pred_act = [act(i) for i in y_pred]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            auc_result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(auc_result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if acc_metric > best_metric:\n",
    "                best_metric = acc_metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), 'best_metric_model.pth')\n",
    "                print('saved new best metric model')\n",
    "            print(f\"current epoch: {epoch + 1} current AUC: {auc_result:.4f}\"\n",
    "                  f\" current accuracy: {acc_metric:.4f} best AUC: {best_metric:.4f}\"\n",
    "                  f\" at epoch: {best_metric_epoch}\")\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30890b64-cbcf-4b93-87ab-10955aff3313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e3440-f599-475e-93d9-02f98a6aba26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
